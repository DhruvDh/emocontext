{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io, sys, os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import torchtext\n",
    "\n",
    "from jupyterthemes import jtplot\n",
    "jtplot.style()\n",
    "\n",
    "sys.path.insert(0, '..\\helpers')\n",
    "from helpers import *\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/train.txt', sep='\\t')\n",
    "dataset['turn1'] = dataset['turn1'].apply(normalize)\n",
    "dataset['turn2'] = dataset['turn2'].apply(normalize)\n",
    "dataset['turn3'] = dataset['turn3'].apply(normalize)\n",
    "dataset.to_csv('train_norm.csv', index=False)\n",
    "data = Dataset('../data/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.preprocessing.text import Tokenizer\n",
    "# tokenizer = Tokenizer(num_words=None, filters='')\n",
    "# tokenizer.fit_on_texts(dataset['turn1'].tolist() + dataset['turn2'].tolist() + dataset['turn3'].tolist())\n",
    "# word_index = tokenizer.word_index\n",
    "\n",
    "\n",
    "_text = torchtext.data.Field(\n",
    "#     include_lengths = True,\n",
    "#     pad_token = ' ',\n",
    "    tokenize = lambda x: x.split()\n",
    ")\n",
    "\n",
    "_label = torchtext.data.Field(is_target = True)\n",
    "\n",
    "_data = torchtext.data.TabularDataset(\n",
    "    path = 'train_norm.csv',\n",
    "    format = 'csv',\n",
    "    fields = [\n",
    "        ('id', None),\n",
    "        ('turn1', _text),\n",
    "        ('turn2', _text),\n",
    "        ('turn3', _text),\n",
    "        ('label', _label)\n",
    "    ],\n",
    "    skip_header = True\n",
    ")\n",
    "\n",
    "_label.build_vocab(_data, specials_first=False)\n",
    "_text.build_vocab(_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no custom vector for  <unk>\n",
      "no custom vector for  <pad>\n",
      "no custom vector for  <unk>\n",
      "no custom vector for  <pad>\n"
     ]
    }
   ],
   "source": [
    "custom_embeddings = get_embedding_matrix(dict(_text.vocab.stoi), 'custom_vectors_V2.5_2500.txt', 100).float()\n",
    "fasttext_embeddings = get_embedding_matrix(dict(_text.vocab.stoi), 'vectors.txt', 300).float()\n",
    "\n",
    "batch_size = 128\n",
    "train, test = _data.split(split_ratio=0.8)\n",
    "train_batches = torchtext.data.batch(\n",
    "    train,\n",
    "    batch_size = batch_size,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, turn1_len, turn2_len, turn3_len = make_tensors(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, fasttext_matrix, custom_matrix):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.fasttext = nn.Embedding(\n",
    "            fasttext_matrix.shape[0],\n",
    "            fasttext_matrix.shape[1],\n",
    "            padding_idx = 0\n",
    "        ).from_pretrained(fasttext_matrix, freeze=False)\n",
    "        \n",
    "        self.custom = nn.Embedding(\n",
    "            custom_matrix.shape[0],\n",
    "            custom_matrix.shape[1],\n",
    "            padding_idx = 0\n",
    "        ).from_pretrained(custom_matrix, freeze=False)\n",
    "#         self.fasttext = nn.Embedding.from_pretrained(fasttext_matrix, freeze=False)\n",
    "#         self.custom = nn.Embedding.from_pretrained(custom_matrix, freeze=False)\n",
    "        \n",
    "        def new_lstm():\n",
    "            return nn.LSTM(\n",
    "                    fasttext_matrix.shape[1] + custom_matrix.shape[1],\n",
    "                    (fasttext_matrix.shape[1] + custom_matrix.shape[1]) * 2,\n",
    "                    num_layers = 2,\n",
    "                    dropout = 0.5,\n",
    "                    bidirectional = True,\n",
    "                    batch_first = True\n",
    "                )\n",
    "        \n",
    "        self.bi_lstm1 = new_lstm()\n",
    "        self.bi_lstm2 = new_lstm()\n",
    "        self.bi_lstm3 = new_lstm() \n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                (fasttext_matrix.shape[1] + custom_matrix.shape[1]) * 12,\n",
    "                int((fasttext_matrix.shape[1] + custom_matrix.shape[1]) * 3)\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(int((fasttext_matrix.shape[1] + custom_matrix.shape[1]) * 3), 4)\n",
    "        )\n",
    "        \n",
    "    def forward(self, turn1, turn2, turn3):\n",
    "        turn1_embeddings = torch.cat((self.fasttext(turn1), self.custom(turn1)), 2)\n",
    "        turn2_embeddings = torch.cat((self.fasttext(turn2), self.custom(turn2)), 2)\n",
    "        turn3_embeddings = torch.cat((self.fasttext(turn3), self.custom(turn3)), 2)\n",
    "        \n",
    "        _out = torch.cat(\n",
    "            (self.bi_lstm1(turn1_embeddings)[0][:, -1, :],\n",
    "            self.bi_lstm2(turn2_embeddings)[0][:, -1, :],\n",
    "            self.bi_lstm3(turn3_embeddings)[0][:, -1, :]), \n",
    "            1\n",
    "        )\n",
    "        \n",
    "        out = self.final(_out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(fasttext_embeddings, custom_embeddings)\n",
    "\n",
    "loss = nn.CrossEntropyLoss(weight=get_class_weights(data.data['label']).to(device))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor_dataset = torch.utils.data.TensorDataset(torch.tensor(data.clone().detach(), dtype=torch.long))\n",
    "train, test = torch.utils.data.random_split(data, (int(0.8*len(data)), int(0.2*len(data))))\n",
    "\n",
    "train_batches = torch.utils.data.DataLoader(\n",
    "    train,\n",
    "    batch_size = 48,\n",
    "    shuffle = True,\n",
    "    pin_memory = True,\n",
    "    num_workers = 1\n",
    ")\n",
    "\n",
    "test_batches = torch.utils.data.DataLoader(\n",
    "    test,\n",
    "    batch_size = 48,\n",
    "    shuffle = True,\n",
    "    pin_memory = True,\n",
    "    num_workers = 1)\n",
    "\n",
    "model.to(device)\n",
    "live_loss = PlotLosses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                                                                            | 0/503 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Traceback (most recent call last):\n  File \"c:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"c:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"c:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 103, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"..\\helpers\\helpers.py\", line 181, in __getitem__\n    return self.data[self.data['id']==index].values[0].tolist()\n  File \"c:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\ops.py\", line 1283, in wrapper\n    res = na_op(values, other)\n  File \"c:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\ops.py\", line 1169, in na_op\n    raise TypeError(\"invalid type comparison\")\nTypeError: invalid type comparison\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c70fc19c572f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mtrain_errors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m         \u001b[0mt1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mt2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    977\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m    978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 979\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    980\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m                 \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    635\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    636\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 637\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    639\u001b[0m     \u001b[0mnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[1;31m# Python 2 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    656\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    657\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 658\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    659\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: Traceback (most recent call last):\n  File \"c:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"c:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 138, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"c:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\utils\\data\\dataset.py\", line 103, in __getitem__\n    return self.dataset[self.indices[idx]]\n  File \"..\\helpers\\helpers.py\", line 181, in __getitem__\n    return self.data[self.data['id']==index].values[0].tolist()\n  File \"c:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\ops.py\", line 1283, in wrapper\n    res = na_op(values, other)\n  File \"c:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\pandas\\core\\ops.py\", line 1169, in na_op\n    raise TypeError(\"invalid type comparison\")\nTypeError: invalid type comparison\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    train_errors = []\n",
    "    for batch in tqdm(train_batches):\n",
    "        t1 = data.tokenizer.texts_to_sequences(batch[1])\n",
    "        t2 = data.tokenizer.texts_to_sequences(batch[2])\n",
    "        t3 = data.tokenizer.texts_to_sequences(batch[3])\n",
    "        \n",
    "        t1 = torch.stack([torch.tensor(x).long() for x in pad_sequences(t1)]).to(device)\n",
    "        t2 = torch.stack([torch.tensor(x).long() for x in pad_sequences(t2)]).to(device)\n",
    "        t3 = torch.stack([torch.tensor(x).long() for x in pad_sequences(t3)]).to(device)\n",
    "#         t1 = batch[0].narrow(1, 0, turn1_len).to(device)\n",
    "#         t2 = batch[0].narrow(1, turn1_len, turn2_len).to(device)\n",
    "#         t3 = batch[0].narrow(1, turn1_len + turn2_len, turn3_len).to(device)\n",
    "\n",
    "#         y = batch[0].narrow(1, turn1_len + turn2_len + turn3_len, 1).to(device)\n",
    "        y = [data.label_tokenizer.word_index[x] - 1 for x in batch[4]]\n",
    "        y = torch.tensor(labels).long().unsqueeze(1).to(device)\n",
    "        \n",
    "        pred = model(t1, t2, t3)\n",
    "        error = loss(pred, y.view(-1))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        error.backward()\n",
    "        optimizer.step()\n",
    "        train_errors.append(error.data.item())\n",
    "        \n",
    "    test_errors = []\n",
    "    for batch in test_batches:\n",
    "        t1 = batch[0].narrow(1, 0, turn1_len).to(device)\n",
    "        t2 = batch[0].narrow(1, turn1_len, turn2_len).to(device)\n",
    "        t3 = batch[0].narrow(1, turn1_len + turn2_len, turn3_len).to(device)\n",
    "\n",
    "        y = batch[0].narrow(1, turn1_len + turn2_len + turn3_len, 1).to(device)\n",
    "        \n",
    "        pred = model(t1, t2, t3)\n",
    "        error = loss(pred, y.view(-1))\n",
    "        \n",
    "        test_errors.append(error.data.item())\n",
    "    \n",
    "    predict = lambda x: torch.argmax(x, dim=1).cpu().numpy()\n",
    "    test_predictions = []\n",
    "    test_actual = []\n",
    "    for batch in test_batches:\n",
    "        t1 = batch[0].narrow(1, 0, turn1_len).to(device)\n",
    "        t2 = batch[0].narrow(1, turn1_len, turn2_len).to(device)\n",
    "        t3 = batch[0].narrow(1, turn1_len + turn2_len, turn3_len).to(device)\n",
    "\n",
    "        y = batch[0].narrow(1, turn1_len + turn2_len + turn3_len, 1).to(device).view(-1)\n",
    "        pred = model(t1, t2, t3)\n",
    "        test_predictions.append(predict(pred))\n",
    "        test_errors.append(loss(pred, y.to(device)).data.item())    # get loss\n",
    "        test_actual.append(y.cpu().numpy())\n",
    "    \n",
    "#     train_predictions = []\n",
    "#     train_actual = []\n",
    "#     for batch in train_batches:\n",
    "#         t1 = batch[0].narrow(1, 0, turn1_len).to(device)\n",
    "#         t2 = batch[0].narrow(1, turn1_len, turn2_len).to(device)\n",
    "#         t3 = batch[0].narrow(1, turn1_len + turn2_len, turn3_len).to(device)\n",
    "\n",
    "#         y = batch[0].narrow(1, turn1_len + turn2_len + turn3_len, 1).to(device).view(-1)\n",
    "#         pred = model(t1, t2, t3)\n",
    "#         train_predictions.append(predict(pred))\n",
    "#         train_actual.append(y.cpu().numpy())\n",
    "        \n",
    "    live_loss.update({\n",
    "        'train_loss': torch.tensor(train_errors).mean(),\n",
    "        'test_loss': torch.tensor(test_errors).mean(),\n",
    "#         'train_f1': f1_score(np.concatenate(train_actual), np.concatenate(train_predictions), average='weighted'),\n",
    "        'test_f1': f1_score(np.concatenate(test_actual), np.concatenate(test_predictions), average='weighted'),\n",
    "    })\n",
    "    live_loss.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_batches))\n",
    "t1 = batch[0].narrow(1, 0, turn1_len).to(device)\n",
    "t2 = batch[0].narrow(1, turn1_len, turn2_len).to(device)\n",
    "t3 = batch[0].narrow(1, turn1_len + turn2_len, turn3_len).to(device)\n",
    "\n",
    "y = batch[0].narrow(1, turn1_len + turn2_len + turn3_len, 1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_t1 = model.fasttext(t1)\n",
    "_t2 = model.custom(t1)\n",
    "_ = torch.cat((_t1, _t2), dim=2)\n",
    "model.bi_lstm1(_.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0] = (torch.rand((32, 240)) * 100).long()\n",
    "\n",
    "t1 = batch[0].narrow(1, 0, turn1_len).to(device)\n",
    "t2 = batch[0].narrow(1, turn1_len, turn2_len).to(device)\n",
    "t3 = batch[0].narrow(1, turn1_len + turn2_len, turn3_len).to(device)\n",
    "\n",
    "y = batch[0].narrow(1, turn1_len + turn2_len + turn3_len, 1).to(device)\n",
    "em1 = nn.Embedding(16366, 300)\n",
    "em1.weight = nn.Parameter(fasttext_embeddings.float())\n",
    "em1.to(device)\n",
    "em2 = nn.Embedding(16366, 100)\n",
    "em2.weight = nn.Parameter(custom_embeddings)\n",
    "em2.to(device)\n",
    "_t1 = em1((torch.rand((32, 40)) * 100).long().to(device))\n",
    "_t2 = em2((torch.rand((32, 40)) * 100).long().to(device))\n",
    "_ = torch.cat((_t1, _t2), dim=2)\n",
    "l1 = model.bi_lstm1(_.to(device))[0]\n",
    "l2 = model.bi_lstm2(_.to(device))[0]\n",
    "l3 = model.bi_lstm3(_.to(device))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_class_weights(dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.savetxt(\"fasttext.csv\", fasttext_embeddings.numpy(), delimiter=\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[ dataset['id'] <= 5].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batches = torch.utils.data.DataLoader(\n",
    "    data,\n",
    "    batch_size = 48,\n",
    "    shuffle = True,\n",
    "    pin_memory = True,\n",
    "    num_workers = 8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 88, 700,  76, 152])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_text.numericalize([next(iter(train_batches))[0].turn1]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.tokenizer.texts_to_sequences(['all the best'])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<list_iterator at 0x1d3d4e2ae80>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iter((next(iter(train_batches))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
